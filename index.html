<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent Lesson</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        /* CSS Styles */
        body {
            font-family: 'Roboto', sans-serif;
            background-color: #ffffff;
            color: #333;
            margin: 0;
            padding: 20px;
            line-height: 1.6;
        }

        .lesson-container {
            max-width: 700px;
            margin: auto;
            padding: 20px;
            background-color: #ffffff;
        }

        h2 {
            color: #1a73e8;
            font-size: 1.5em;
        }

        button {
            padding: 10px 20px;
            margin-top: 20px;
            background-color: #000;
            color: #fff;
            font-weight: bold;
            border: none;
            border-radius: 20px;
            font-size: 1em;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        button:focus {
            outline: none;
        }

        .hidden-content {
            visibility: hidden;
            min-height: 100px;
        }

        .formula {
            padding: 10px 0;
            margin: 15px 0;
            font-family: monospace;
        }

        .placeholder-image {
            height: 150px;
            background-color: #e0e7ff;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 10px;
            margin: 20px 0;
            font-size: 1em;
            color: #6b7280;
        }
    </style>
</head>
<body>
    <div class="lesson-container">
        <!-- Introduction -->
        <section id="intro" class="lesson-section">
            <div class="placeholder-image">Image Placeholder</div>
            <p>Imagine you're hiking down a mountain in thick fog. You can't see the path ahead, but you can feel the slope beneath your feet. How do you ensure you're heading toward the valley below and not off a cliff?</p>
            <p>In such a situation, a sensible strategy is to feel the steepest downward slope and take a small step in that direction. By repeatedly doing this, you'd gradually make your way to the bottom.</p>
            <p>This intuitive approach is the essence of gradient descent—a fundamental algorithm in optimization and machine learning used to find the minimum of a function.</p>
            <button onclick="showNextSection('gradientSection', this)">Continue</button>
        </section>

        <!-- The Gradient Section -->
        <section id="gradientSection" class="lesson-section hidden-content">
            <h2>The Gradient: Your Directional Guide</h2>
            <p>First, let's understand the concept of a gradient. In simple terms, it's like a compass pointing in the direction of the steepest ascent of a function.</p>
            <div class="placeholder-image">Placeholder Image</div>
            <p>Mathematically, for a function \( f(x) \) in one dimension, the gradient is the derivative \( f'(x) \).</p>
            <div class="formula">
                In multiple dimensions, the gradient is a vector of partial derivatives:
                \[
                \nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)
                \]
            </div>
            <p>This vector points in the direction where the function increases the most.</p>
            <div class="placeholder-image">Placeholder Visual Aid</div>
            <button onclick="showNextSection('steppingSection', this)">Continue</button>
        </section>

        <!-- Stepping Towards the Minimum Section -->
        <section id="steppingSection" class="lesson-section hidden-content">
            <h2>Stepping Towards the Minimum</h2>
            <p>To minimize a function, we do the opposite—we move in the direction of the steepest descent.</p>
            <div class="formula">
                The basic update rule in gradient descent is:
                \[
                x_{\text{new}} = x_{\text{old}} - \alpha \nabla f(x_{\text{old}})
                \]
                Where:
                <ul>
                    <li>\( x_{\text{old}} \): Current position.</li>
                    <li>\( \alpha \): Learning rate, a small positive number controlling the step size.</li>
                    <li>\( \nabla f(x_{\text{old}}) \): Gradient at the current position.</li>
                </ul>
            </div>
            <button onclick="showNextSection('practiceSection', this)">Continue</button>
        </section>

        <!-- Practice Section -->
        <section id="practiceSection" class="lesson-section hidden-content">
            <h2>Let's put this into practice!</h2>
            <p>Suppose we have the function \( f(x) = x^2 \). Starting from \( x_0 = 4 \) and using a learning rate \( \alpha = 0.1 \).</p>
            <div class="formula">
                Calculate the gradient at \( x_0 \):
                \[
                f'(x) = 2x
                \]
                \[
                f'(4) = 2 \times 4 = 8
                \]
                Update \( x \):
                \[
                x_1 = x_0 - \alpha f'(x_0) = 4 - 0.1 \times 8 = 3.2
                \]
                Answer: After one iteration, \( x = 3.2 \).
            </div>
            <button onclick="showNextSection('twoDSection', this)">Continue</button>
        </section>

        <!-- Two Dimensions Section -->
        <section id="twoDSection" class="lesson-section hidden-content">
            <h2>Extending to Two Dimensions</h2>
            <p>Now, let's explore gradient descent in two dimensions.</p>
            <div class="formula">
                Function:
                \[
                f(x, y) = x^2 + y^2
                \]
            </div>
            <p>This function represents a bowl-shaped surface with a minimum at \( (0, 0) \).</p>
            <div class="placeholder-image">Placeholder Visual Aid</div>
            <p>Starting from \( (x_0, y_0) = (3, 4) \) and using \( \alpha = 0.1 \).</p>
            <div class="formula">
                Compute the gradients:
                \[
                \frac{\partial f}{\partial x} = 2x \implies \frac{\partial f}{\partial x}(3, 4) = 6
                \]
                \[
                \frac{\partial f}{\partial y} = 2y \implies \frac{\partial f}{\partial y}(3, 4) = 8
                \]
                Update \( x \) and \( y \):
                \[
                x_1 = x_0 - \alpha \frac{\partial f}{\partial x} = 3 - 0.1 \times 6 = 2.4
                \]
                \[
                y_1 = y_0 - \alpha \frac{\partial f}{\partial y} = 4 - 0.1 \times 8 = 3.2
                \]
                Answer: After one iteration, \( (x, y) = (2.4, 3.2) \).
            </div>
            <button onclick="showNextSection('summarySection', this)">Continue</button>
        </section>

        <!-- Summary Section -->
        <section id="summarySection" class="lesson-section hidden-content">
            <h2>Summary and Key Takeaways</h2>
            <p>Gradient Descent is an iterative method to find the minimum of a function by moving in the opposite direction of the gradient.</p>
            <ul>
                <li>The learning rate \( \alpha \) controls the step size.</li>
                <li>If \( \alpha \) is too large, the steps may overshoot the minimum.</li>
                <li>If \( \alpha \) is too small, convergence is slow.</li>
                <li>In higher dimensions, we deal with gradient vectors.</li>
            </ul>
            <p>In the next lesson, we'll explore how to calculate gradients and update trainable parameters in regression models, building on what you've learned and showing you how gradient descent is used in real-world machine learning scenarios.</p>
        </section>
    </div>

    <script>
        // JavaScript for Interactivity
        function showNextSection(sectionId, button) {
            document.getElementById(sectionId).style.visibility = 'visible';
            button.style.display = 'none';
        }
    </script>
</body>
</html>
